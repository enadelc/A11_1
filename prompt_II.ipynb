{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERVIEW**\n",
    "\n",
    "In this application, you will explore a dataset from kaggle. The original dataset contained information on 3 million used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRISP-DM Framework\n",
    "\n",
    "<center>\n",
    "    <img src = images/crisp.png width = 50%/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To frame the task, throughout our practical applications we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "\n",
    "From a business perspective, we are tasked with identifying key drivers for used car prices.  In the CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few sentences, reframe the task as a data task with the appropriate technical vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To address the business task, we will construct a regression model to predict used car prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The model will analyze features such as vehicle age, mileage, make, model, and condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "By examining the relationships between these features and the sale prices, we aim to identify the most significant factors driving price variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The objective is to translate these insights into actionable strategies for pricing and inventory management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To understand the dataset,we start by loading and inspecting its structure, checking for any missing values, data types, and duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Perform exploratory data analysis (EDA) using visualizations to identify patterns, distributions, and correlations between features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Assess the data quality by detecting outliers, inconsistencies, and potential inaccuracies that could impact the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Finally, determine if the dataset is suitable for modeling or if additional data is needed to address any gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "After our initial exploration and fine tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean the dataset by addressing any remaining missing values, correcting outliers, and ensuring data integrity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encode categorical variables and split the data into training and testing sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('data/vehicles.csv')\n",
    "# Remove NaN values\n",
    "data.dropna(inplace=True)\n",
    "# Initial data inspection\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data.describe(include='all'))\n",
    "\n",
    "# Data Cleaning\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.dropna(subset=['price', 'year', 'manufacturer', 'model', 'condition', 'odometer'], inplace=True)  # Drop rows with essential missing values\n",
    "\n",
    "# Feature Engineering\n",
    "data['age'] = 2024 - data['year']\n",
    "data['mileage_per_year'] = data['odometer'] / data['age']\n",
    "\n",
    "# Define features and target variable\n",
    "X = data[['age', 'mileage_per_year', 'manufacturer', 'model', 'condition', 'cylinders', 'fuel', 'drive', 'size', 'type', 'paint_color', 'state', 'transmission']]\n",
    "y = data['price']\n",
    "\n",
    "# Data Transformation\n",
    "numeric_features = ['age', 'mileage_per_year']\n",
    "categorical_features = ['manufacturer', 'model', 'condition', 'fuel', 'drive', 'size', 'type', 'paint_color', 'state', 'transmission', 'cylinders']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Build multiple regression models, including Linear Regression, Random Forest, and Gradient Boosting, to predict car prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Explore and optimize different hyperparameters using GridSearchCV or RandomizedSearchCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Apply k-fold cross-validation to ensure consistent performance across different data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                         ('regressor', LinearRegression())]),  # Set the number of features to 8\n",
    "    'Gradient Boosting': Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                         ('regressor', GradientBoostingRegressor(n_estimators=512, max_depth=100,max_features=8))]),  # Set the number of trees to 100 and max depth to 3    \n",
    "    'Random Forest': Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('regressor', RandomForestRegressor(n_estimators=512, max_depth=100,max_features=8))])  # Set the number of trees to 512 and max depth to 10}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "With some modeling accomplished, we aim to reflect on what we identify as a high quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight on drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluate the models by comparing their performance metrics against the business objective of identifying key drivers for used car prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyze how well the models provide insights into which features most influence price and assess their predictive accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Reflect on whether the results meet the business goals or if there are areas needing further refinement or exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Based on this assessment, decide if any earlier phases of data preparation or modeling need revisiting or if the findings are ready to be presented to the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {'MSE': mse, 'R2': r2}\n",
    "    \n",
    "    print(f\"{name} - MSE: {mse}, R2: {r2}\")\n",
    "\n",
    "# Model Comparison Plot\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "# Plot R2 on the right axis\n",
    "ax = results_df.plot(kind='bar', figsize=(10, 6))\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(results_df.index, results_df['R2'], color='red', marker='o')\n",
    "ax2.set_ylabel('R2')\n",
    "plt.title('Model Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine tuning their inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It seems that the model Gradient Boosting is the best model based on the MSE and R2. Numerous iterations of parameters where tried with all three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print top five important features for each model\n",
    "for name, model in models.items():\n",
    "    if name == 'Linear Regression':\n",
    "        feature_importances = model.named_steps['regressor'].coef_\n",
    "        feature_names = model.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "        top_features = pd.Series(feature_importances, index=numeric_features + list(feature_names))\n",
    "        top_features = top_features.abs().sort_values(ascending=False).head(5)\n",
    "        print(f\"Top five important features for {name}:\")\n",
    "        print(top_features)\n",
    "        print()\n",
    "    elif name == 'K-Nearest Neighbors':\n",
    "        print(f\"Top five important features for {name}:\")\n",
    "        print(\"K-Nearest Neighbors does not provide feature importances\")\n",
    "        print()\n",
    "    else:\n",
    "        feature_importances = model.named_steps['regressor'].feature_importances_\n",
    "        feature_names = model.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "        top_features = pd.Series(feature_importances, index=numeric_features + list(feature_names))\n",
    "        top_features = top_features.abs().sort_values(ascending=False).head(5)\n",
    "        print(f\"Top five important features for {name}:\")\n",
    "        print(top_features)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The results of the findings are as follows below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear Regression - MSE: 92038296.64641926, R2: 0.5127299385145626\n",
    "Gradient Boosting - MSE: 45754142.68705401, R2: 0.7577679647203505\n",
    "Random Forest - MSE: 89977228.91472253, R2: 0.5236416637087793"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "At the same time it seems that Random Forest and Gradient Boosting had the best results to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top five important features for Linear Regression:\n",
    "model_benz sprinter           114278.431500\n",
    "model_challenger srt demon    101050.294715\n",
    "model_5 window coupe           88114.664943\n",
    "model_850i                     84044.609071\n",
    "model_hot rod                  83465.557273\n",
    "dtype: float64\n",
    "\n",
    "Top five important features for Gradient Boosting:\n",
    "age                      0.132557\n",
    "mileage_per_year         0.069720\n",
    "cylinders_8 cylinders    0.030959\n",
    "cylinders_8 cylinders    0.030959\n",
    "fuel_diesel              0.030813\n",
    "type_truck               0.029214\n",
    "dtype: float64\n",
    "type_truck               0.029214\n",
    "dtype: float64\n",
    "dtype: float64\n",
    "\n",
    "Top five important features for Random Forest:\n",
    "age                 0.098791\n",
    "mileage_per_year    0.061506\n",
    "fuel_diesel         0.044064\n",
    "fuel_gas            0.041248\n",
    "type_truck          0.032176\n",
    "dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["It seems that age and mileage have the best indicator on car price. We recomend targeting these for buying and selling from the dealer point of view."]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
